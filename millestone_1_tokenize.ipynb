{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1EdpEYKohez",
        "outputId": "6c36c0ad-be9c-4ee9-9113-88cc9c6361a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a statement: hello + world\n",
            "Tokens: ['hello', '+', 'world']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def tokenize_input(self, statement):\n",
        "        tokens = []\n",
        "        # Regular expression to identify tokens\n",
        "        pattern = r'[a-zA-Z]+|\\d+|\"[a-zA-Z0-9\\s]+\"|\\+|substring|\\(|\\)|length|\\='\n",
        "        matches = re.findall(pattern, statement)\n",
        "        for match in matches:\n",
        "            tokens.append(match)\n",
        "        return tokens\n",
        "\n",
        "# Example usage:\n",
        "tokenizer = Tokenizer()\n",
        "statement = input(\"Enter a statement: \")\n",
        "tokens = tokenizer.tokenize_input(statement)\n",
        "print(\"Tokens:\", tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJdoMv8Uo-NQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}